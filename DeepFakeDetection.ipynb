{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#!pip install \"../input/mtcnndetect/mtcnn-0.1.0-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "import tensorflow.keras.backend as k\n",
    "from tensorflow.keras.models import Sequential,load_model,Model\n",
    "from tensorflow.keras.layers import  Conv2D , MaxPooling2D , Activation,UpSampling2D,Flatten,Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "#from mtcnn import MTCNN\n",
    "import random\n",
    "#BATCH_SIZE = 64\n",
    "IMG_WIDTH = 150\n",
    "IMG_HEIGHT = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, ZeroPadding2D,Lambda\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "tf.disable_eager_execution()\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.io.gfile.GFile('../input/mobilenetface/frozen_inference_graph_face.pb', 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "        config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess=tf.compat.v1.Session(graph=detection_graph, config=config)\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    boxes_tensor = detection_graph.get_tensor_by_name('detection_boxes:0')    \n",
    "    scores_tensor = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobilenet_face(image):\n",
    "    global boxes,scores,num_detections\n",
    "    (im_height,im_width)=image.shape[:-1]\n",
    "    imgs=np.array([image])\n",
    "    (boxes, scores) = sess.run(\n",
    "        [boxes_tensor, scores_tensor],\n",
    "        feed_dict={image_tensor: imgs})\n",
    "    max_=np.where(scores==scores.max())[0][0]\n",
    "    box=boxes[0][max_]\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                ymin * im_height, ymax * im_height)\n",
    "    left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n",
    "    return (left, right, top, bottom)\n",
    "def crop_image(frame,bbox):\n",
    "    left, right, top, bottom=bbox\n",
    "    return frame[top:bottom,left:right]\n",
    "def get_img(frame):\n",
    "    return cv2.resize(crop_image(frame,get_mobilenet_face(frame)),(IMG_WIDTH,IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo_model = FaceDetector()\n",
    "#detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_(filename,model,interval=4):\n",
    "    preds = []\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    v_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_n = 0\n",
    "    m = -3\n",
    "    #lists = [0, 1, 2, 3, 7, 19, 31, 43, 59, 73, 97, 109, 127, 139, 157, 167, 181, 211, 223]\n",
    "    lists = np.linspace(0,v_len,interval, endpoint=False, dtype=np.int)\n",
    "    #lists = random.sample(range(5,35),k=1)\n",
    "    while(cap.isOpened()):\n",
    "        ret,frame = cap.read() #ret is a boolean variable that returns true if the frame is available\n",
    "        \n",
    "        if (frame_n in lists)  :\n",
    "            if not ret:\n",
    "                break\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            try:\n",
    "                newm = get_img(image)\n",
    "                pred0 = np.float16(model.predict([[newm/127.5 -1]]).clip(0.05,0.95)[0][0])\n",
    "                pred1 = np.float16(model.predict([[cv2.flip(newm,-1)/127.5 -1]]).clip(0.05,0.95)[0][0])\n",
    "                pred = 0.5*pred0 + 0.5*pred1\n",
    "            except:\n",
    "                pred = 0.5\n",
    "            preds += [pred] \n",
    "            #if (pred + m)/2 > 0.6:\n",
    "             #   break\n",
    "            #elif pred > 0.7:\n",
    "             #   m = (pred + m) /2\n",
    "            #else:\n",
    "             #   m = m - 0.2\n",
    "            if len(preds) >= interval:\n",
    "                break\n",
    "        frame_n += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnet(x, beta=1.0, gama=0.0):\n",
    "    return  k.sigmoid(beta * (x-gama))\n",
    "class Psnet(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, beta=1.0, gama=0.0, **kwargs):\n",
    "        super(Psnet, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.beta = beta\n",
    "        self.gama = gama\n",
    "#        self.num_outputs = num_outputs\n",
    "        self.__name__ = 'Psnet'\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#        self.beta = k.variable(self.beta,\n",
    "#                                      dtype=k.floatx(),\n",
    "#                                      name='beta_factor')\n",
    "#        self.gama = k.variable(self.gama,\n",
    "#                                      dtype=k.floatx(),\n",
    "#                                      name='gama_factor')\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                        dtype=k.floatx(),\n",
    "                                        shape=[1,1],\n",
    "                                        initializer='uniform',\n",
    "                                        trainable=True,\n",
    "                                        constraint=None)\n",
    "        self.gama = self.add_weight(name='gama',\n",
    "                                        dtype=k.floatx(),\n",
    "                                        shape=[1,1],\n",
    "                                        initializer='uniform',\n",
    "                                        trainable=True,\n",
    "                                        constraint=None)\n",
    "\n",
    "        super(Psnet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        return psnet(inputs, self.beta, self.gama)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'beta': self.get_weights()[0],\n",
    "                  'gama': self.get_weights()[1]} \n",
    "        base_config = super(Psnet, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.random_normal_initializer(0., 0.02)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D , BatchNormalization, AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Activation\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam    \n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "#from keras.applications.nasnet import NASNetMobile  \n",
    "base_model = Xception(input_shape=(IMG_WIDTH,IMG_HEIGHT,3), include_top=False, weights=None)\n",
    "#add SE layer for Middle Xception block\n",
    "#########################################################################\n",
    "layer_c = base_model.layers[44]\n",
    "f = Model(inputs=base_model.input, outputs=layer_c.output)\n",
    "layer_b = f.layers[-10].output\n",
    "init = 44\n",
    "for index in [55,65,75,85,95,105,115,132]:\n",
    "    for layer in range(init,index):\n",
    "        if layer > init+1:\n",
    "            if layer == 122:\n",
    "                layer_c1 = base_model.layers[layer](layer_b) \n",
    "            elif layer == 124:\n",
    "                layer_c1 = base_model.layers[layer](layer_c1)\n",
    "            elif layer == 125:\n",
    "                layer_c = base_model.layers[layer]([layer_c,layer_c1])\n",
    "            else:\n",
    "                layer_c = base_model.layers[layer](layer_c)\n",
    "        elif layer in [44,54,64,74,84,94,104,114]:\n",
    "            #layer_a = layer_c\n",
    "            dims = f.output_shape\n",
    "            a = f.output\n",
    "            a_se = AveragePooling2D(pool_size=dims[1:3],padding='valid')(a)\n",
    "            a_se = Conv2D(int(dims[3]/16),[1,1],activation=\"relu\")(a_se)\n",
    "            a_se = Conv2D(dims[3],[1,1])(a_se)\n",
    "            a_se = Psnet(beta=1.0,gama=0.0)(a_se)\n",
    "            a_tot = tf.multiply(a,a_se)\n",
    "        elif layer in [45,55,65,75,85,95,105,115]:\n",
    "            layer_b = base_model.layers[layer]([a_tot,layer_b])\n",
    "            layer_c = layer_b\n",
    "    f = Model(inputs= f.input, outputs=layer_c)    \n",
    "    init = index -1         \n",
    "###########################################################\n",
    "#print(block_layer)\n",
    "#print(add_layer)\n",
    "#assert(len(block_layer)==len(add_layer))\n",
    "for layer in f.layers:\n",
    "    layer.trainable = True\n",
    "im = f.output\n",
    "im = GlobalAveragePooling2D()(im)\n",
    "x = Dense(2048,kernel_initializer=initializer,kernel_regularizer=tf.keras.regularizers.l2(0.01),activation=\"relu\")(im)\n",
    "#x = Activation('relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Dense(128,kernel_initializer=initializer,kernel_regularizer=tf.keras.regularizers.l2(0.01),activation=\"relu\")(im)\n",
    "#x = Activation('relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "prediction = Dense(1)(x)\n",
    "prediction = Psnet(beta=1.0,gama=0.0)(prediction)\n",
    "model = Model(inputs=f.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/kaggle/input/modelandw/final_w2.hdf5')\n",
    "#model = load_model('/kaggle/input/modelandw/XPsnet-SE-Psnet.hdf5',custom_objects={'Psnet':Psnet})\n",
    "#latest = tf.train.latest_checkpoint('/kaggle/input/modelandw/')\n",
    "#latest += '.data-00000-of-00001'\n",
    "#model.load_weights(latest)\n",
    "#model.summary()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
    "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
    "#len(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths= [test_dir + x for x in test_videos]\n",
    "#paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "max_iteration_duration = 8.1\n",
    "safe = 60\n",
    "start = time.time()\n",
    "aa=[]\n",
    "for j in paths:\n",
    "    try:\n",
    "        if time.time() - start > (len(paths) * max_iteration_duration) - safe :\n",
    "            raise Exception('Time is up predicting 0.5')\n",
    "        #print(j)    \n",
    "        \n",
    "        preds =  get_preds_(j,model,interval=20)\n",
    "        if len(preds) == 0:\n",
    "            print('nothing here')\n",
    "            aa += [0.5]\n",
    "        else:\n",
    "            #preds = preds.clip(0.01,0.99)\n",
    "            z = sum(preds)/len(preds)\n",
    "            #z = max(preds)\n",
    "            aa  +=[np.float16(z)]\n",
    "        #print(aa)\n",
    "    except Exception as e:\n",
    "        aa  += [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": aa})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([46., 46., 33., 29., 51., 29., 21., 38., 29., 78.]),\n",
       " array([0.0506897 , 0.14064026, 0.23059082, 0.32054138, 0.41049194,\n",
       "        0.5004425 , 0.59039307, 0.68034363, 0.77029419, 0.86024475,\n",
       "        0.95019531]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(submission_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.515137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.117371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.666992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.106140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.698730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename     label\n",
       "0  aassnaulhq.mp4  0.515137\n",
       "1  aayfryxljh.mp4  0.117371\n",
       "2  acazlolrpz.mp4  0.666992\n",
       "3  adohdulfwb.mp4  0.106140\n",
       "4  ahjnxtiamx.mp4  0.698730"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
